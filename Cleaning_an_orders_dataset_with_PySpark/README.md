# Cleaning an Orders Dataset with PySpark  

This project demonstrates how to efficiently load, transform, and analyze large datasets using Apache Spark and Pandas. The primary focus is on reading Parquet files, converting data between Spark and Pandas, and conducting basic data exploration for further analysis.  

## Overview  
This project highlights the process of handling and exploring data using a combination of Spark and Pandas, serving as a practical example for data engineers and data scientists who work with large datasets stored in Parquet format.  

## Features  
1. Load data from Parquet files using Apache Spark.  
2. Convert Spark DataFrames to Pandas DataFrames for streamlined exploration.  
3. Perform basic data exploration and preview datasets to gain initial insights.  

## Getting Started  
To start this project:  
1. Download and install any version of Jupyter Notebook compatible with Python.  
2. Download the attached dataset (`orders_data.parquet`).  
3. All necessary connections and dependencies are included in the provided script.  

---

